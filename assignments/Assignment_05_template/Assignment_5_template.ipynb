{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a382e79c2764685",
      "metadata": {
        "id": "2a382e79c2764685"
      },
      "source": [
        "# Assignment 5: Construction Equipment Detection using YOLOv5\n",
        "\n",
        "### Overview\n",
        "In this assignment, you will build an object detection model to detect construction vehicles and machines using the dataset from [Kaggle](https://www.kaggle.com/datasets/kartaviychert/arh-df)\n",
        "\n",
        "## Dataset Summary\n",
        "\n",
        "**Construction Equipment** is a dataset for an object detection task. Possible applications of the dataset could be in the construction and surveillance industries.\n",
        "\n",
        "The dataset consists of 318 images with **3,752** labeled objects belonging to **5** different classes including `crane`, `excavator`, `truck`, and ` tractor` and `other`.\n",
        "\n",
        "Images in the Construction Equipment dataset have bounding box annotations. All images are labeled (i.e. with annotations). There are no pre-defined train/val/test splits in the dataset. The dataset was released in 2023.\n",
        "\n",
        "The dataset is structured as follows:\n",
        "  - **img/**: Contains all JPG images\n",
        "  - **ann/**: Contains annotation files in JSON format (one per image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98fce1d4-c70d-4bf7-bcbb-2fb35bbcadc7",
      "metadata": {
        "id": "98fce1d4-c70d-4bf7-bcbb-2fb35bbcadc7"
      },
      "source": [
        "You will:\n",
        "1. Download and unzip the dataset from a Google Drive shared link.\n",
        "2. Split the dataset into training (80%), validation (10%), and test (10%) sets.\n",
        "3. Convert JSON annotations to YOLO format (.txt files) and save them in a new `labels` folder for each split.\n",
        "4. Create a YOLOv5 configuration file.\n",
        "5. Train a YOLOv5 model (using pretrained YOLOv5s weights) and monitor performance.\n",
        "6. Evaluate the model on the test set (computing precision, recall, and mAP).\n",
        "7. Run inference and display a few detected result images.\n",
        "\n",
        "> **Note:** Update `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your actual Google Drive file ID. This notebook is designed to run in Google Colab with GPU enabled."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87a1e5c32abd5c",
      "metadata": {
        "id": "a87a1e5c32abd5c"
      },
      "source": [
        "## Step 1: Download the Dataset\n",
        "\n",
        "We will use `gdown` to download the dataset from a shared Google Drive link. The dataset should unzip into a folder named `ARH-DF` containing two folders:\n",
        "\n",
        "```\n",
        "ARH-DF/\n",
        "   ├── img/         (all .jpg images)\n",
        "   └── ann/         (annotation files in JSON format)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c659916a451ade0f",
      "metadata": {
        "id": "c659916a451ade0f"
      },
      "outputs": [],
      "source": [
        "!pip install gdown pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8642fd3314fc15e9",
      "metadata": {
        "id": "8642fd3314fc15e9"
      },
      "source": [
        "### Download Dataset from Google Drive\n",
        "Replace `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your actual file ID.\n",
        "\n",
        "**Dataset link**: https://drive.google.com/file/d/1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp/view\n",
        "\n",
        "**Google Drive file ID**: 1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c29315b6263bbff6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c29315b6263bbff6",
        "outputId": "7fba9f7d-04cb-45d4-b1e6-1a8aa8fe85d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp\n",
            "From (redirected): https://drive.google.com/uc?id=1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp&confirm=t&uuid=53436bad-eaca-4176-adf8-310068536e1a\n",
            "To: /content/construction_equipment_dataset.zip\n",
            "100%|██████████| 427M/427M [00:02<00:00, 207MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and unzipped to /content/construction_equipment_dataset\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "# Replace with your actual Google Drive file ID for the zipped dataset\n",
        "file_id = '1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output_zip = '/content/construction_equipment_dataset.zip'\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "gdown.download(url, output_zip, quiet=False)\n",
        "\n",
        "!unzip -q /content/construction_equipment_dataset.zip -d /content/construction_equipment_dataset\n",
        "print(\"Dataset downloaded and unzipped to /content/construction_equipment_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7865f4c94a08ae9",
      "metadata": {
        "id": "b7865f4c94a08ae9"
      },
      "source": [
        "## Step 2: Split the Dataset and Convert Annotations\n",
        "\n",
        "The raw dataset in `/content/construction_equipment_dataset` has two folders: `img` for images and `ann` for JSON annotation files. We'll split the images into train (80%), validation (10%), and test (10%) sets. Then, for each image, we'll convert its corresponding JSON annotation to YOLO format and save it as a `.txt` file in a new `labels` folder for that split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3db1c2c85f8b2124",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3db1c2c85f8b2124",
        "outputId": "8948e8fc-a058-43db-ca5f-7a0d1e4a4e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 318\n"
          ]
        }
      ],
      "source": [
        "import os, shutil, random, json\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define raw dataset directories\n",
        "base_dir = '/content/construction_equipment_dataset'\n",
        "orig_images_dir = os.path.join(base_dir, 'img')\n",
        "orig_ann_dir = os.path.join(base_dir, 'ann')  # JSON annotations\n",
        "\n",
        "# Create directories for train, val, and test splits (each with 'images' and 'labels')\n",
        "splits = ['train', 'val', 'test']\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_dir, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, split, 'labels'), exist_ok=True)\n",
        "\n",
        "# List all image files (assuming .jpg)\n",
        "all_images = [f for f in os.listdir(orig_images_dir) if f.endswith('.jpg')]\n",
        "total_images = len(all_images)\n",
        "print(f\"Total images found: {total_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c6f2a6-d6f7-432e-894c-e469fefcce3d",
      "metadata": {
        "id": "43c6f2a6-d6f7-432e-894c-e469fefcce3d"
      },
      "source": [
        "#### Shuffle and split images: 80% train, 10% val, 10% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e743afce-f3a6-47cd-bd9d-30dbabbd13ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e743afce-f3a6-47cd-bd9d-30dbabbd13ea",
        "outputId": "b65030a3-9866-4e7f-c384-23ca4af7a630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits -> Train: 254, Val: 32, Test: 32\n"
          ]
        }
      ],
      "source": [
        "# Shuffle and split images: 80% train, 10% val, 10% test\n",
        "random.shuffle(all_images)\n",
        "train_end = int(0.8 * total_images)\n",
        "val_end = int(0.9 * total_images)\n",
        "train_imgs = all_images[:train_end]\n",
        "val_imgs = all_images[train_end:val_end]\n",
        "test_imgs = all_images[val_end:]\n",
        "print(f\"Dataset splits -> Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dccdec-df5d-422a-a705-d48f257d3339",
      "metadata": {
        "id": "b3dccdec-df5d-422a-a705-d48f257d3339"
      },
      "source": [
        "### [Important!!!] Define class names (update as needed, e.g., 'vehicle', 'machine')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before setting up the training, let's scan the JSON annotation files (in `ann`) to print out all unique class names. Update the `classes` variable accordingly."
      ],
      "metadata": {
        "id": "uPmKXCcymSzD"
      },
      "id": "uPmKXCcymSzD"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Gather all JSON files in the annotation folder\n",
        "json_files = glob.glob(os.path.join(orig_ann_dir, '*.json'))\n",
        "\n",
        "unique_classes = set()\n",
        "\n",
        "for jf in json_files:\n",
        "    with open(jf, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # In this dataset, annotations are under the key 'objects' and class names are stored in 'classTitle'\n",
        "    for obj in data.get(\"objects\", []):\n",
        "        cls = obj.get(\"classTitle\")\n",
        "        if cls:\n",
        "            unique_classes.add(cls)\n",
        "\n",
        "print(\"Unique class names found in the dataset:\")\n",
        "for cls in sorted(unique_classes):\n",
        "    print(\" -\", cls)\n",
        "\n",
        "# Now update the classes variable (e.g., if the dataset contains excavator, truck)\n",
        "classes = sorted(unique_classes)\n",
        "print(\"Updated classes:\", classes)"
      ],
      "metadata": {
        "id": "rUQWqYrEkI_m",
        "outputId": "1a042a17-829b-4758-c07b-620b27efb9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rUQWqYrEkI_m",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique class names found in the dataset:\n",
            " - crane\n",
            " - excavator\n",
            " - other\n",
            " - tractor\n",
            " - truck\n",
            "Updated classes: ['crane', 'excavator', 'other', 'tractor', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "67f6bd84-8032-4551-b23b-9c726894fece",
      "metadata": {
        "id": "67f6bd84-8032-4551-b23b-9c726894fece"
      },
      "outputs": [],
      "source": [
        "# Define class names (update as needed, e.g., 'vehicle', 'machine')\n",
        "classes = ['crane', 'excavator', 'other', 'tractor', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bb4a8a90-d6f4-4680-891e-b49182a92017",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb4a8a90-d6f4-4680-891e-b49182a92017",
        "outputId": "28edf8bf-2a2f-4c8e-b804-c85efa38f1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split and JSON annotations converted to YOLO format (labels folder).\n"
          ]
        }
      ],
      "source": [
        "def convert_json_to_yolo(json_file, classes, image_file):\n",
        "    \"\"\"\n",
        "    Expected JSON structure:\n",
        "    {\n",
        "        \"size\": {\"width\": ..., \"height\": ...},\n",
        "        \"objects\": [\n",
        "             {\"classTitle\": \"excavator\", \"points\": {\"exterior\": [[xmin, ymin], [xmax, ymax]]}, ...},\n",
        "             ...\n",
        "        ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # Get image dimensions\n",
        "    if \"size\" in data and \"width\" in data[\"size\"] and \"height\" in data[\"size\"]:\n",
        "        width = float(data[\"size\"][\"width\"])\n",
        "        height = float(data[\"size\"][\"height\"])\n",
        "    else:\n",
        "        img = cv2.imread(image_file)\n",
        "        height, width = img.shape[:2]\n",
        "\n",
        "    yolo_lines = []\n",
        "    for obj in data.get(\"objects\", []):\n",
        "        cls = obj.get(\"classTitle\")\n",
        "        if cls not in classes:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        # Assume bounding box is in \"points\"->\"exterior\" with two points: top-left and bottom-right\n",
        "        points = obj.get(\"points\", {}).get(\"exterior\", [])\n",
        "        if len(points) < 2:\n",
        "            continue\n",
        "        xmin, ymin = points[0]\n",
        "        xmax, ymax = points[1]\n",
        "        x_center = ((xmin + xmax) / 2) / width\n",
        "        y_center = ((ymin + ymax) / 2) / height\n",
        "        bbox_width = (xmax - xmin) / width\n",
        "        bbox_height = (ymax - ymin) / height\n",
        "        yolo_lines.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\")\n",
        "    return yolo_lines\n",
        "\n",
        "def copy_and_convert_files(file_list, src_images, src_ann, dest_split):\n",
        "    dest_img = os.path.join(base_dir, dest_split, 'images')\n",
        "    dest_lbl = os.path.join(base_dir, dest_split, 'labels')\n",
        "    for fname in file_list:\n",
        "        # Copy image\n",
        "        shutil.copy(os.path.join(src_images, fname), dest_img)\n",
        "        # For annotation, use the full filename with '.json' appended\n",
        "        json_file = os.path.join(src_ann, fname + '.json')\n",
        "        image_file = os.path.join(src_images, fname)\n",
        "        if os.path.exists(json_file):\n",
        "            yolo_lines = convert_json_to_yolo(json_file, classes, image_file)\n",
        "            if yolo_lines:\n",
        "                with open(os.path.join(dest_lbl, fname + '.txt'), 'w') as f:\n",
        "                    f.write(\"\\n\".join(yolo_lines))\n",
        "        else:\n",
        "            print(f\"Warning: No annotation found for {fname}\")\n",
        "\n",
        "copy_and_convert_files(train_imgs, orig_images_dir, orig_ann_dir, 'train')\n",
        "copy_and_convert_files(val_imgs, orig_images_dir, orig_ann_dir, 'val')\n",
        "copy_and_convert_files(test_imgs, orig_images_dir, orig_ann_dir, 'test')\n",
        "print(\"Dataset split and JSON annotations converted to YOLO format (labels folder).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "81d77724-15b1-457b-9527-992159ec1499",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81d77724-15b1-457b-9527-992159ec1499",
        "outputId": "632091c8-18be-4f79-aec1-ab8e54a6f007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Images: 254, Labels: 254\n",
            "Val - Images: 32, Labels: 32\n",
            "Test - Images: 32, Labels: 32\n"
          ]
        }
      ],
      "source": [
        "for split in splits:\n",
        "    images_count = len(os.listdir(os.path.join(base_dir, split, 'images')))\n",
        "    labels_count = len(os.listdir(os.path.join(base_dir, split, 'labels')))\n",
        "    print(f\"{split.capitalize()} - Images: {images_count}, Labels: {labels_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6c1f1d6a178020",
      "metadata": {
        "id": "da6c1f1d6a178020"
      },
      "source": [
        "## Step 3: Create YOLOv5 Dataset Configuration\n",
        "\n",
        "Create a YAML configuration file (`construction_equipment.yaml`) for YOLOv5 that points to your training and validation sets and lists the class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7244402906220b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7244402906220b1",
        "outputId": "bb11ea7d-5bdd-4787-8173-e7ff68b0b08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created YOLOv5 dataset configuration file: construction_equipment.yaml\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "config_data = {\n",
        "    'train': os.path.join(base_dir, 'train', 'images'),\n",
        "    'val': os.path.join(base_dir, 'val', 'images'),\n",
        "    'test': os.path.join(base_dir, 'test', 'images'),\n",
        "    'names': classes,\n",
        "    'nc': len(classes)\n",
        "}\n",
        "\n",
        "with open('construction_equipment.yaml', 'w') as f:\n",
        "    yaml.dump(config_data, f)\n",
        "print(\"Created YOLOv5 dataset configuration file: construction_equipment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d507ea93af3e1863",
      "metadata": {
        "id": "d507ea93af3e1863"
      },
      "source": [
        "## Step 4: Set Up YOLOv5 Environment\n",
        "\n",
        "Clone the YOLOv5 repository and install dependencies. This is designed to run in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4a1cf3c908505137",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a1cf3c908505137",
        "outputId": "9ded47df-3640-4a7f-d788-4b4ee4763be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab? True\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17270, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 17270 (delta 0), reused 0 (delta 0), pack-reused 17269 (from 2)\u001b[K\n",
            "Receiving objects: 100% (17270/17270), 16.12 MiB | 17.94 MiB/s, done.\n",
            "Resolving deltas: 100% (11858/11858), done.\n",
            "/content/yolov5/yolov5\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (8.3.75)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(\"Running in Colab?\", IN_COLAB)\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/ultralytics/yolov5.git\n",
        "    %cd yolov5\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"Ensure YOLOv5 is installed locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41cea8b3512353cc",
      "metadata": {
        "id": "41cea8b3512353cc"
      },
      "source": [
        "## Step 5: Train YOLOv5 Model\n",
        "\n",
        "Train the model using pretrained YOLOv5s weights. Adjust epochs and batch size as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221f9eb06484f640",
      "metadata": {
        "id": "221f9eb06484f640"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Starting training...\")\n",
        "    !python train.py --img 640 --batch 16 --epochs 10 --data /content/construction_equipment.yaml --weights yolov5s.pt --name yolo_construction_equipment_exp\n",
        "else:\n",
        "    print(\"Run the YOLOv5 training command in your local environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d559eb935b992be9",
      "metadata": {
        "id": "d559eb935b992be9"
      },
      "source": [
        "## Step 6: Evaluate Model Performance on Test Set\n",
        "\n",
        "Since YOLOv5's `val.py` does not support a custom 'test' key by default, we create a temporary YAML config that uses the test set as the validation set. This allows us to compute performance metrics (precision, recall, mAP) on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57636afda9932514",
      "metadata": {
        "id": "57636afda9932514"
      },
      "outputs": [],
      "source": [
        "temp_config = {\n",
        "    'train': os.path.join(base_dir, 'train', 'images'),  # dummy\n",
        "    'val': os.path.join(base_dir, 'test', 'images'),       # use test set for evaluation\n",
        "    'names': classes,\n",
        "    'nc': len(classes)\n",
        "}\n",
        "\n",
        "with open('temp_test.yaml', 'w') as f:\n",
        "    yaml.dump(temp_config, f)\n",
        "print(\"Created temporary YAML config (temp_test.yaml) for test evaluation.\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Running validation on test subset (using /content/temp_test.yaml)...\")\n",
        "    !python val.py --data /content/temp_test.yaml --weights runs/train/yolo_construction_equipment_exp/weights/best.pt --img 640 --conf 0.25 --half --save-json\n",
        "else:\n",
        "    print(\"Run the validation command in your local environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c756c6c242957845",
      "metadata": {
        "id": "c756c6c242957845"
      },
      "source": [
        "### Load and Display Evaluation Metrics\n",
        "Load the evaluation metrics from the JSON file generated by YOLOv5 and print standard metrics (precision, recall, mAP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550b95676314a4c7",
      "metadata": {
        "id": "550b95676314a4c7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "metrics_file = '/content/yolov5/runs/val/exp/results.json'\n",
        "if os.path.exists(metrics_file):\n",
        "    with open(metrics_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "    print(\"\\nTest Set Evaluation Metrics:\")\n",
        "    print(\"Precision:\", metrics.get(\"precision\", \"N/A\"))\n",
        "    print(\"Recall:\", metrics.get(\"recall\", \"N/A\"))\n",
        "    print(\"mAP@0.5:\", metrics.get(\"mAP50\", \"N/A\"))\n",
        "    print(\"mAP@0.5:0.95:\", metrics.get(\"mAP50-95\", \"N/A\"))\n",
        "else:\n",
        "    print(\"Metrics JSON file not found. Check the validation output folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad07d2672c92e3f",
      "metadata": {
        "id": "dad07d2672c92e3f"
      },
      "source": [
        "## Step 7: Inference on Test Set and Visualize Detected Results\n",
        "\n",
        "Run inference on the test set with YOLOv5 using the `--save-txt` flag (to save predicted YOLO annotations) and display the detection result images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dbf222a36537691",
      "metadata": {
        "id": "2dbf222a36537691"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Running detection on test set (with --save-txt)...\")\n",
        "    !python detect.py --weights runs/train/yolo_construction_equipment_exp/weights/best.pt --img 640 --conf 0.25 --save-txt --source ../construction_equipment_dataset/test/img --name test_inference\n",
        "else:\n",
        "    print(\"Run the detection command in your local environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c274e804d9ae11ae",
      "metadata": {
        "id": "c274e804d9ae11ae"
      },
      "source": [
        "### Display Detected Results\n",
        "Display a few detected result images (with bounding boxes drawn) from the YOLOv5 output folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e28b2f99ce5fe88",
      "metadata": {
        "id": "6e28b2f99ce5fe88"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "detected_results_dir = '/content/yolov5/runs/detect/test_inference/exp'\n",
        "detected_image_files = glob.glob(os.path.join(detected_results_dir, '*.jpg'))\n",
        "print(f\"Found {len(detected_image_files)} detected result images.\")\n",
        "\n",
        "for img_path in detected_image_files[:5]:\n",
        "    display(Image(filename=img_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168ea5c231fc0719",
      "metadata": {
        "id": "168ea5c231fc0719"
      },
      "source": [
        "## Step 8: Tips for Improvement & Next Steps\n",
        "\n",
        "1. Use data augmentation (e.g., Albumentations) to further increase training diversity.\n",
        "2. Experiment with longer training and hyperparameter tuning (e.g., learning rate, batch size, mosaic augmentation).\n",
        "3. Consider integrating the trained model with live camera feeds for real-time monitoring.\n",
        "4. Analyze incorrect predictions to iterate on model improvements.\n",
        "\n",
        "Happy Coding and Stay Safe on Site!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}