{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a382e79c2764685",
      "metadata": {
        "id": "2a382e79c2764685"
      },
      "source": [
        "# Assignment 5: Construction Equipment Detection using YOLOv5\n",
        "\n",
        "### Overview\n",
        "In this assignment, you will build an object detection model to detect construction vehicles and machines using the dataset from [Kaggle](https://www.kaggle.com/datasets/kartaviychert/arh-df)\n",
        "\n",
        "## Dataset Summary\n",
        "\n",
        "**Construction Equipment** is a dataset for an object detection task. Possible applications of the dataset could be in the construction and surveillance industries.\n",
        "\n",
        "The dataset consists of 318 images with **3,752** labeled objects belonging to **5** different classes including `crane`, `excavator`, `truck`, and ` tractor` and `other`.\n",
        "\n",
        "Images in the Construction Equipment dataset have bounding box annotations. All images are labeled (i.e. with annotations). There are no pre-defined train/val/test splits in the dataset. The dataset was released in 2023.\n",
        "\n",
        "The dataset is structured as follows:\n",
        "  - **img/**: Contains all JPG images\n",
        "  - **ann/**: Contains annotation files in JSON format (one per image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98fce1d4-c70d-4bf7-bcbb-2fb35bbcadc7",
      "metadata": {
        "id": "98fce1d4-c70d-4bf7-bcbb-2fb35bbcadc7"
      },
      "source": [
        "You will:\n",
        "1. Download and unzip the dataset from a Google Drive shared link.\n",
        "2. Split the dataset into training (80%), validation (10%), and test (10%) sets.\n",
        "3. Convert JSON annotations to YOLO format (.txt files) and save them in a new `labels` folder for each split.\n",
        "4. Create a YOLOv5 configuration file.\n",
        "5. Train a YOLOv5 model (using pretrained YOLOv5s weights) and monitor performance.\n",
        "6. Evaluate the model on the test set (computing precision, recall, and mAP).\n",
        "7. Run inference and display a few detected result images.\n",
        "\n",
        "> **Note:** Update `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your actual Google Drive file ID. This notebook is designed to run in Google Colab with GPU enabled."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87a1e5c32abd5c",
      "metadata": {
        "id": "a87a1e5c32abd5c"
      },
      "source": [
        "## Step 1: Download the Dataset\n",
        "\n",
        "We will use `gdown` to download the dataset from a shared Google Drive link. The dataset should unzip into a folder named `ARH-DF` containing two folders:\n",
        "\n",
        "```\n",
        "ARH-DF/\n",
        "   ├── img/         (all .jpg images)\n",
        "   └── ann/         (annotation files in JSON format)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c659916a451ade0f",
      "metadata": {
        "id": "c659916a451ade0f"
      },
      "outputs": [],
      "source": [
        "!pip install gdown pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8642fd3314fc15e9",
      "metadata": {
        "id": "8642fd3314fc15e9"
      },
      "source": [
        "### Download Dataset from Google Drive\n",
        "Replace `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your actual file ID.\n",
        "\n",
        "**Dataset link**: https://drive.google.com/file/d/1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp/view\n",
        "\n",
        "**Google Drive file ID**: 1gbfkO37WZl4cVxE-YhGFxqMwSDL5Xyzp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29315b6263bbff6",
      "metadata": {
        "id": "c29315b6263bbff6"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Replace with your actual Google Drive file ID for the zipped dataset\n",
        "file_id = '<YOUR_GOOGLE_DRIVE_FILE_ID>'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output_zip = '/content/construction_equipment_dataset.zip'\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "gdown.download(url, output_zip, quiet=False)\n",
        "\n",
        "!unzip -q /content/construction_equipment_dataset.zip -d /content/construction_equipment_dataset\n",
        "print(\"Dataset downloaded and unzipped to /content/construction_equipment_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7865f4c94a08ae9",
      "metadata": {
        "id": "b7865f4c94a08ae9"
      },
      "source": [
        "## Step 2: Split the Dataset and Convert Annotations\n",
        "\n",
        "The raw dataset in `/content/construction_equipment_dataset` has two folders: `img` for images and `ann` for JSON annotation files. We'll split the images into train (80%), validation (10%), and test (10%) sets. Then, for each image, we'll convert its corresponding JSON annotation to YOLO format and save it as a `.txt` file in a new `labels` folder for that split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db1c2c85f8b2124",
      "metadata": {
        "id": "3db1c2c85f8b2124"
      },
      "outputs": [],
      "source": [
        "import os, shutil, random, json\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define raw dataset directories\n",
        "base_dir = '/content/construction_equipment_dataset'\n",
        "orig_images_dir = os.path.join(base_dir, 'img')\n",
        "orig_ann_dir = os.path.join(base_dir, 'ann')  # JSON annotations\n",
        "\n",
        "# Create directories for train, val, and test splits (each with 'images' and 'labels')\n",
        "splits = ['train', 'val', 'test']\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_dir, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, split, 'labels'), exist_ok=True)\n",
        "\n",
        "# List all image files (assuming .jpg)\n",
        "all_images = [f for f in os.listdir(orig_images_dir) if f.endswith('.jpg')]\n",
        "total_images = len(all_images)\n",
        "print(f\"Total images found: {total_images}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c6f2a6-d6f7-432e-894c-e469fefcce3d",
      "metadata": {
        "id": "43c6f2a6-d6f7-432e-894c-e469fefcce3d"
      },
      "source": [
        "#### Shuffle and split images: 80% train, 10% val, 10% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e743afce-f3a6-47cd-bd9d-30dbabbd13ea",
      "metadata": {
        "id": "e743afce-f3a6-47cd-bd9d-30dbabbd13ea"
      },
      "outputs": [],
      "source": [
        "# Shuffle and split images: 80% train, 10% val, 10% test\n",
        "random.shuffle(all_images)\n",
        "train_end = int(0.8 * total_images)\n",
        "val_end = int(0.9 * total_images)\n",
        "train_imgs = all_images[:train_end]\n",
        "val_imgs = all_images[train_end:val_end]\n",
        "test_imgs = all_images[val_end:]\n",
        "print(f\"Dataset splits -> Train: {len(train_imgs)}, Val: {len(val_imgs)}, Test: {len(test_imgs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dccdec-df5d-422a-a705-d48f257d3339",
      "metadata": {
        "id": "b3dccdec-df5d-422a-a705-d48f257d3339"
      },
      "source": [
        "### [Important!!!] Define class names (update as needed, e.g., 'vehicle', 'machine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f6bd84-8032-4551-b23b-9c726894fece",
      "metadata": {
        "id": "67f6bd84-8032-4551-b23b-9c726894fece"
      },
      "outputs": [],
      "source": [
        "# Define class names (update as needed, e.g., 'vehicle', 'machine')\n",
        "classes = ['vehicle', 'machine']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4a8a90-d6f4-4680-891e-b49182a92017",
      "metadata": {
        "id": "bb4a8a90-d6f4-4680-891e-b49182a92017"
      },
      "outputs": [],
      "source": [
        "def convert_json_to_yolo(json_file, classes):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # Assumes JSON has 'size' and 'objects' keys similar to VOC structure\n",
        "    width = float(data['size']['width'])\n",
        "    height = float(data['size']['height'])\n",
        "    yolo_lines = []\n",
        "    for obj in data['objects']:\n",
        "        cls = obj['name']\n",
        "        if cls not in classes:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        bndbox = obj['bndbox']\n",
        "        xmin = float(bndbox['xmin'])\n",
        "        ymin = float(bndbox['ymin'])\n",
        "        xmax = float(bndbox['xmax'])\n",
        "        ymax = float(bndbox['ymax'])\n",
        "        x_center = ((xmin + xmax) / 2) / width\n",
        "        y_center = ((ymin + ymax) / 2) / height\n",
        "        bbox_width = (xmax - xmin) / width\n",
        "        bbox_height = (ymax - ymin) / height\n",
        "        yolo_lines.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\")\n",
        "    return yolo_lines\n",
        "\n",
        "def copy_and_convert_files(file_list, src_images, src_ann, dest_split):\n",
        "    dest_img = os.path.join(base_dir, dest_split, 'images')\n",
        "    dest_lbl = os.path.join(base_dir, dest_split, 'labels')\n",
        "    for fname in file_list:\n",
        "        shutil.copy(os.path.join(src_images, fname), dest_img)\n",
        "        base_name = os.path.splitext(fname)[0]\n",
        "        json_file = os.path.join(src_ann, base_name + '.json')\n",
        "        if os.path.exists(json_file):\n",
        "            yolo_lines = convert_json_to_yolo(json_file, classes)\n",
        "            if yolo_lines:\n",
        "                with open(os.path.join(dest_lbl, base_name + '.txt'), 'w') as f:\n",
        "                    f.write(\"\\n\".join(yolo_lines))\n",
        "        else:\n",
        "            print(f\"Warning: No annotation found for {fname}\")\n",
        "\n",
        "copy_and_convert_files(train_imgs, orig_images_dir, orig_ann_dir, 'train')\n",
        "copy_and_convert_files(val_imgs, orig_images_dir, orig_ann_dir, 'val')\n",
        "copy_and_convert_files(test_imgs, orig_images_dir, orig_ann_dir, 'test')\n",
        "print(\"Dataset split and JSON annotations converted to YOLO format (labels folder).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d77724-15b1-457b-9527-992159ec1499",
      "metadata": {
        "id": "81d77724-15b1-457b-9527-992159ec1499"
      },
      "outputs": [],
      "source": [
        "for split in splits:\n",
        "    images_count = len(os.listdir(os.path.join(base_dir, split, 'images')))\n",
        "    labels_count = len(os.listdir(os.path.join(base_dir, split, 'labels')))\n",
        "    print(f\"{split.capitalize()} - Images: {images_count}, Labels: {labels_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6c1f1d6a178020",
      "metadata": {
        "id": "da6c1f1d6a178020"
      },
      "source": [
        "## Step 3: Create YOLOv5 Dataset Configuration\n",
        "\n",
        "Create a YAML configuration file (`construction_equipment.yaml`) for YOLOv5 that points to your training and validation sets and lists the class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7244402906220b1",
      "metadata": {
        "id": "7244402906220b1"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "config_data = {\n",
        "    'train': os.path.join(base_dir, 'train', 'images'),\n",
        "    'val': os.path.join(base_dir, 'val', 'images'),\n",
        "    'names': classes,\n",
        "    'nc': len(classes)\n",
        "}\n",
        "\n",
        "with open('construction_equipment.yaml', 'w') as f:\n",
        "    yaml.dump(config_data, f)\n",
        "print(\"Created YOLOv5 dataset configuration file: construction_equipment.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d507ea93af3e1863",
      "metadata": {
        "id": "d507ea93af3e1863"
      },
      "source": [
        "## Step 4: Set Up YOLOv5 Environment\n",
        "\n",
        "Clone the YOLOv5 repository and install dependencies. This is designed to run in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1cf3c908505137",
      "metadata": {
        "id": "4a1cf3c908505137"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(\"Running in Colab?\", IN_COLAB)\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/ultralytics/yolov5.git\n",
        "    %cd yolov5\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"Ensure YOLOv5 is installed locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41cea8b3512353cc",
      "metadata": {
        "id": "41cea8b3512353cc"
      },
      "source": [
        "## Step 5: Train YOLOv5 Model\n",
        "\n",
        "Train the model using pretrained YOLOv5s weights. Adjust epochs and batch size as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221f9eb06484f640",
      "metadata": {
        "id": "221f9eb06484f640"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Starting training...\")\n",
        "    !python train.py --img 640 --batch 16 --epochs 10 --data ../construction_equipment.yaml --weights yolov5s.pt --name yolo_construction_equipment_exp\n",
        "else:\n",
        "    print(\"Run the YOLOv5 training command in your local environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d559eb935b992be9",
      "metadata": {
        "id": "d559eb935b992be9"
      },
      "source": [
        "## Step 6: Evaluate Model Performance on Test Set\n",
        "\n",
        "Since YOLOv5's `val.py` does not support a custom 'test' key by default, we create a temporary YAML config that uses the test set as the validation set. This allows us to compute performance metrics (precision, recall, mAP) on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57636afda9932514",
      "metadata": {
        "id": "57636afda9932514"
      },
      "outputs": [],
      "source": [
        "temp_config = {\n",
        "    'train': os.path.join(base_dir, 'train', 'images'),  # dummy\n",
        "    'val': os.path.join(base_dir, 'test', 'images'),       # use test set for evaluation\n",
        "    'names': classes,\n",
        "    'nc': len(classes)\n",
        "}\n",
        "\n",
        "with open('temp_test.yaml', 'w') as f:\n",
        "    yaml.dump(temp_config, f)\n",
        "print(\"Created temporary YAML config (temp_test.yaml) for test evaluation.\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Running validation on test subset (using /content/temp_test.yaml)...\")\n",
        "    !python val.py --data /content/temp_test.yaml --weights runs/train/yolo_construction_equipment_exp/weights/best.pt --img 640 --conf 0.25 --half --save-json\n",
        "else:\n",
        "    print(\"Run the validation command in your local environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c756c6c242957845",
      "metadata": {
        "id": "c756c6c242957845"
      },
      "source": [
        "### Load and Display Evaluation Metrics\n",
        "Load the evaluation metrics from the JSON file generated by YOLOv5 and print standard metrics (precision, recall, mAP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550b95676314a4c7",
      "metadata": {
        "id": "550b95676314a4c7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "metrics_file = '/content/yolov5/runs/val/exp/results.json'\n",
        "if os.path.exists(metrics_file):\n",
        "    with open(metrics_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "    print(\"\\nTest Set Evaluation Metrics:\")\n",
        "    print(\"Precision:\", metrics.get(\"precision\", \"N/A\"))\n",
        "    print(\"Recall:\", metrics.get(\"recall\", \"N/A\"))\n",
        "    print(\"mAP@0.5:\", metrics.get(\"mAP50\", \"N/A\"))\n",
        "    print(\"mAP@0.5:0.95:\", metrics.get(\"mAP50-95\", \"N/A\"))\n",
        "else:\n",
        "    print(\"Metrics JSON file not found. Check the validation output folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dad07d2672c92e3f",
      "metadata": {
        "id": "dad07d2672c92e3f"
      },
      "source": [
        "## Step 7: Inference on Test Set and Visualize Detected Results\n",
        "\n",
        "Run inference on the test set with YOLOv5 using the `--save-txt` flag (to save predicted YOLO annotations) and display the detection result images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dbf222a36537691",
      "metadata": {
        "id": "2dbf222a36537691"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Running detection on test set (with --save-txt)...\")\n",
        "    !python detect.py --weights runs/train/yolo_construction_equipment_exp/weights/best.pt --img 640 --conf 0.25 --save-txt --source ../construction_equipment_dataset/test/img --name test_inference\n",
        "else:\n",
        "    print(\"Run the detection command in your local environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c274e804d9ae11ae",
      "metadata": {
        "id": "c274e804d9ae11ae"
      },
      "source": [
        "### Display Detected Results\n",
        "Display a few detected result images (with bounding boxes drawn) from the YOLOv5 output folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e28b2f99ce5fe88",
      "metadata": {
        "id": "6e28b2f99ce5fe88"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "detected_results_dir = '/content/yolov5/runs/detect/test_inference/exp'\n",
        "detected_image_files = glob.glob(os.path.join(detected_results_dir, '*.jpg'))\n",
        "print(f\"Found {len(detected_image_files)} detected result images.\")\n",
        "\n",
        "for img_path in detected_image_files[:5]:\n",
        "    display(Image(filename=img_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168ea5c231fc0719",
      "metadata": {
        "id": "168ea5c231fc0719"
      },
      "source": [
        "## Step 8: Tips for Improvement & Next Steps\n",
        "\n",
        "1. Use data augmentation (e.g., Albumentations) to further increase training diversity.\n",
        "2. Experiment with longer training and hyperparameter tuning (e.g., learning rate, batch size, mosaic augmentation).\n",
        "3. Consider integrating the trained model with live camera feeds for real-time monitoring.\n",
        "4. Analyze incorrect predictions to iterate on model improvements.\n",
        "\n",
        "Happy Coding and Stay Safe on Site!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}