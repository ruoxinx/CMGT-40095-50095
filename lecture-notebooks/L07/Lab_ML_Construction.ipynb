{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML in Construction: Extended Practice Notebook**\n",
    "\n",
    "Welcome! This notebook demonstrates **machine learning workflows** in a construction context, using **Concrete Compressive Strength** data as an example.\n",
    "\n",
    "We'll cover:\n",
    "1. **Data Loading & Exploration**\n",
    "2. **Basic Model**: Linear Regression\n",
    "3. **Advanced Models**: Random Forest & XGBoost\n",
    "4. **Hyperparameter Tuning**\n",
    "5. **Discussion of Data Leakage and Best Practices**\n",
    "6. **Pointers to Real-World Data Sources**\n",
    "\n",
    "The methods here apply to many construction-related problems (cost estimation, schedule forecasting, safety classification, etc.), even though we focus on concrete strength data for demonstration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We'll use:\n",
    "- **pandas**, **numpy** for data handling.\n",
    "- **matplotlib**, **seaborn** for visualization.\n",
    "- **scikit-learn**, **xgboost** for machine learning models.\n",
    "\n",
    "> If you haven't installed `xgboost`, you'll need to install it (e.g., `pip install xgboost`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# For XGBoost:\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgboost_installed = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost is not installed. Please install via 'pip install xgboost' if you'd like to run XGBRegressor.\")\n",
    "    xgboost_installed = False\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset\n",
    "\n",
    "We'll use the **Concrete Compressive Strength** dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength). It contains concrete mixtures with 8 input variables and 1 target (compressive strength).\n",
    "\n",
    "Make sure you have `Concrete_Data.csv` in the same directory (or update the path below)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    df = pd.read_csv('Concrete_Data.csv')\n",
    "    print(\"Data loaded from Concrete_Data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find Concrete_Data.csv. Please place it in the same folder or update the file path.\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Info\n",
    "According to many versions of this dataset, columns are often in the order:\n",
    "```\n",
    "1) Cement (component 1) -- quantitative\n",
    "2) Blast Furnace Slag (component 2) -- quantitative\n",
    "3) Fly Ash (component 3) -- quantitative\n",
    "4) Water (component 4) -- quantitative\n",
    "5) Superplasticizer (component 5) -- quantitative\n",
    "6) Coarse Aggregate (component 6) -- quantitative\n",
    "7) Fine Aggregate (component 7) -- quantitative\n",
    "8) Age (day) -- quantitative\n",
    "9) Concrete compressive strength (MPa) -- quantitative\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Inspection & Cleaning\n",
    "We'll look for:\n",
    "- Missing values\n",
    "- Basic statistics\n",
    "- Potential outliers\n",
    "\n",
    "> This dataset typically has no missing values, but let's confirm. Also, we'll rename the last column to 'Strength' if needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if df is not None:\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(\"\\nColumn Names:\", df.columns.tolist())\n",
    "    print(\"\\nMissing Values per Column:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    # Rename the last column to 'Strength' if not already\n",
    "    if df.columns[-1].lower() not in ['strength', 'compressivestrength']:\n",
    "        df.rename(columns={df.columns[-1]: 'Strength'}, inplace=True)\n",
    "\n",
    "    display(df.describe())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Visual Checks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if df is not None:\n",
    "    # Histograms\n",
    "    df.hist(figsize=(12,8), bins=20, color='steelblue', edgecolor='black')\n",
    "    plt.suptitle('Feature Distributions', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(8,6))\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='Blues')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split & Avoiding Data Leakage\n",
    "We'll separate the target variable (**Strength**) from the features. Then we’ll create a train/test split. **Important**: If we decide to scale or transform the features, we do so *after* splitting to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if df is not None:\n",
    "    # Separate features and target\n",
    "    X = df.drop('Strength', axis=1)\n",
    "    y = df['Strength']\n",
    "\n",
    "    # Create a train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    print(f\"Training set: {X_train.shape[0]} rows, Test set: {X_test.shape[0]} rows\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Feature Scaling\n",
    "Some advanced models (like neural networks) benefit significantly from scaling. Tree-based models (like Random Forest, XGBoost) are generally less sensitive, but let's demonstrate it for completeness.\n",
    "\n",
    "> **Note**: We fit the scaler on **X_train** only, then apply the same transform to **X_test**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model: Linear Regression\n",
    "Let's start with a simple baseline model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "lin_reg = LinearRegression()\n",
    "# Fit on scaled data\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "# Predict\n",
    "y_pred_lr = lin_reg.predict(X_test_scaled)\n",
    "# Evaluate\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Linear Regression Performance:\")\n",
    "print(f\"  MAE : {mae_lr:.3f}\")\n",
    "print(f\"  RMSE: {rmse_lr:.3f}\")\n",
    "print(f\"  R^2 : {r2_lr:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Interpretation\n",
    "We have an initial benchmark. Next, we’ll try **more advanced ML models** to see if we can improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Model: Random Forest\n",
    "\n",
    "Random Forest is an **ensemble** of decision trees, often performing very well on tabular data. It can capture non-linear relationships better than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100,\n",
    "                               random_state=42)\n",
    "\n",
    "# Train on scaled data (though for Random Forest, scaling is often less critical)\n",
    "rf_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Performance:\")\n",
    "print(f\"  MAE : {mae_rf:.3f}\")\n",
    "print(f\"  RMSE: {rmse_rf:.3f}\")\n",
    "print(f\"  R^2 : {r2_rf:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Random Forest (and other tree-based models) can provide an estimate of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if df is not None:\n",
    "    importances = rf_reg.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    # Pair and sort\n",
    "    feat_imp = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Feature Importances (Random Forest):\")\n",
    "    for name, imp in feat_imp:\n",
    "        print(f\"  {name}: {imp:.4f}\")\n",
    "\n",
    "    # Let's do a quick bar chart\n",
    "    names, scores = zip(*feat_imp)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=list(scores), y=list(names), palette='viridis')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Model: XGBoost (Optional)\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is another popular, high-performing library for tree-based models. It often beats random forest in speed or performance for many structured datasets. If you have not installed it, you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if xgboost_installed:\n",
    "    xgb_reg = XGBRegressor(random_state=42)\n",
    "    xgb_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_reg.predict(X_test_scaled)\n",
    "    \n",
    "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "    print(\"XGBoost Performance:\")\n",
    "    print(f\"  MAE : {mae_xgb:.3f}\")\n",
    "    print(f\"  RMSE: {rmse_xgb:.3f}\")\n",
    "    print(f\"  R^2 : {r2_xgb:.3f}\")\n",
    "else:\n",
    "    print(\"XGBoost not installed. Skipping XGBRegressor.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "\n",
    "We can improve model performance by tuning key hyperparameters. Let's illustrate with **RandomizedSearchCV** on **RandomForestRegressor**. (We choose random search because it's often more efficient than grid search for multiple parameters.)\n",
    "\n",
    "> In real practice, you'd do this on the full training set, sometimes combining it with cross-validation. Also, be sure to watch out for training time on larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We'll do a quick random search on RandomForestRegressor.\n",
    "# Adjust the param_distributions as needed.\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    scoring='neg_root_mean_squared_error', # negative RMSE (maximize this)\n",
    "    n_iter=5, # for demonstration, keep it small\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # use all available CPU cores\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters from RandomizedSearchCV:\")\n",
    "print(rf_search.best_params_)\n",
    "\n",
    "# Evaluate best model on the test set\n",
    "best_rf = rf_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test_scaled)\n",
    "\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best_rf)\n",
    "rmse_best = mean_squared_error(y_test, y_pred_best_rf, squared=False)\n",
    "r2_best = r2_score(y_test, y_pred_best_rf)\n",
    "\n",
    "print(\"\\nPerformance of Tuned Random Forest:\")\n",
    "print(f\"  MAE : {mae_best:.3f}\")\n",
    "print(f\"  RMSE: {rmse_best:.3f}\")\n",
    "print(f\"  R^2 : {r2_best:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Leakage & Best Practices\n",
    "1. **Train-Test Split Before Transformations**: We scaled features *after* splitting.\n",
    "2. **No Usage of Future Info**: Make sure we do not use any data that would only be available after the target is known.\n",
    "3. **Cross-Validation**: More robust estimation of performance.\n",
    "4. **Feature Engineering**: Domain insights (e.g., ratio of Water/Cement can be important in real design, etc.).\n",
    "5. **Versioning**: In real projects, keep track of data, transformations, and model versions to ensure reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary of Results\n",
    "Let's compare **MAE, RMSE, R²** for each model we tested.\n",
    "\n",
    "- **Linear Regression**\n",
    "- **Random Forest** (default)\n",
    "- **XGBoost** (if installed)\n",
    "- **Tuned Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summarize results\n",
    "results = {\n",
    "    'LinearRegression': {\n",
    "        'MAE': mae_lr,\n",
    "        'RMSE': rmse_lr,\n",
    "        'R2': r2_lr\n",
    "    },\n",
    "    'RandomForest_Default': {\n",
    "        'MAE': mae_rf,\n",
    "        'RMSE': rmse_rf,\n",
    "        'R2': r2_rf\n",
    "    },\n",
    "    'RandomForest_Tuned': {\n",
    "        'MAE': mae_best,\n",
    "        'RMSE': rmse_best,\n",
    "        'R2': r2_best\n",
    "    }\n",
    "}\n",
    "\n",
    "if xgboost_installed:\n",
    "    results['XGBoost'] = {\n",
    "        'MAE': mae_xgb,\n",
    "        'RMSE': rmse_xgb,\n",
    "        'R2': r2_xgb\n",
    "    }\n",
    "\n",
    "# Print the table\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "display(df_results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Real-World Data Sources for Construction\n",
    "\n",
    "1. **UCI ML Repository** – [Concrete Compressive Strength](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength) and other datasets.\n",
    "2. **Kaggle** – Datasets on building permits, civil engineering, or cost estimation.\n",
    "3. **Open Government Data** – Some city/state portals share inspection logs, building permits, etc.\n",
    "4. **Company/Internal Data** – Real site logs, sensor data, project schedules (often proprietary). Combine them with domain knowledge to create robust ML solutions.\n",
    "\n",
    "## 12. Next Steps & Further Practice\n",
    "- **Feature Engineering**: For real projects, domain knowledge is crucial (e.g., water/cement ratio, plasticizer usage, weather data, etc.).\n",
    "- **Other Models**: Try LightGBM, CatBoost, or neural networks.\n",
    "- **Cross-Validation**: For final model validation, use k-fold cross-validation.\n",
    "- **Deployment**: In production, set up model monitoring (MLOps) to track performance over time.\n",
    "\n",
    "Thank you for going through this extended notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
