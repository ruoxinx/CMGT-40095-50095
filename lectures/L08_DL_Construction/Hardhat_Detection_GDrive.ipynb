{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8rVA-y1mqa7"
      },
      "source": [
        "# Hardhat Detection in Construction: Deep Learning Tutorial Using Google Drive\n",
        "\n",
        "Author: *[Your Name]*  \n",
        "Date: *[Today's Date]*\n",
        "\n",
        "## Overview\n",
        "In this notebook, you will:\n",
        "1. Mount your Google Drive and download the Hardhat Detection dataset from a shared Google Drive link using `gdown`.\n",
        "2. Unzip and split the dataset into training, validation, and testing sets.\n",
        "3. Set up YOLOv5 for object detection, configure the dataset, and start training with monitoring.\n",
        "4. Evaluate and test the model using inference on unseen images.\n",
        "\n",
        "> **Note:** This notebook is designed for Google Colab with GPU enabled. Update the Google Drive file ID in the code below."
      ],
      "id": "B8rVA-y1mqa7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXKplBVZmqa8"
      },
      "source": [
        "## 1. Mounting Google Drive & Downloading the Dataset\n",
        "\n",
        "We use `gdown` to download the zipped dataset from a shared Google Drive link. Replace `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your file's ID. The expected folder structure after unzipping is:\n",
        "\n",
        "```\n",
        "HardHat_Dataset/\n",
        "   â”œâ”€â”€ images/\n",
        "   â”‚    â”œâ”€â”€ image_0001.png # All image files (.png)\n",
        "   â”‚    â”œâ”€â”€ ...\n",
        "   â””â”€â”€ annotations/\n",
        "        â”œâ”€â”€ image_0001.xml # Annotation files in VOC XML format\n",
        "        â”œâ”€â”€ ...\n",
        "```"
      ],
      "id": "aXKplBVZmqa8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbXRL0mWmqa9"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "gbXRL0mWmqa9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlAaVUEymqa9",
        "outputId": "7fb06fdd-8ad8-48a7-b7e3-21633d4af61f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install gdown if not already installed\n",
        "!pip install gdown"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "id": "ZlAaVUEymqa9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-SEqfBFmqa9"
      },
      "source": [
        "### Download the Dataset from Google Drive\n",
        "Replace `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your shared file ID. This file should be a zip archive of the dataset."
      ],
      "id": "l-SEqfBFmqa9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saIaG3sumqa-",
        "outputId": "87b9d381-f618-4c06-9c03-a0fb805a3416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gdown\n",
        "\n",
        "# Replace with your file ID from the shared Google Drive link\n",
        "file_id = '1XebIf0c3LDe_KNcPR6KI8Ys4ReNj6knG'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = '/content/hardhat_dataset_1k.zip'\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q /content/hardhat_dataset_1k.zip -d /content/HardHat_Dataset_1k\n",
        "print(\"Dataset downloaded and unzipped to /content/HardHat_Dataset_1k\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1XebIf0c3LDe_KNcPR6KI8Ys4ReNj6knG\n",
            "From (redirected): https://drive.google.com/uc?id=1XebIf0c3LDe_KNcPR6KI8Ys4ReNj6knG&confirm=t&uuid=8b001c6e-e771-4966-98ff-14a36ace6b67\n",
            "To: /content/hardhat_dataset_1k.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 264M/264M [00:02<00:00, 126MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and unzipped to /content/HardHat_Dataset_1k\n"
          ]
        }
      ],
      "id": "saIaG3sumqa-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8tiiYhvmqa-"
      },
      "source": [
        "## 2. Splitting the Dataset into Train, Validation, and Test Sets\n",
        "\n",
        "Since the raw dataset has no predefined splits, we will create training (80%), validation (10%), and test (10%) subsets. This will create the following folder structure:\n",
        "\n",
        "```\n",
        "HardHat_Dataset/\n",
        "   â”œâ”€â”€ images/\n",
        "   â”œâ”€â”€ annotations/\n",
        "   â”œâ”€â”€ train/ (new folder)\n",
        "   â”‚    â”œâ”€â”€ images/\n",
        "   â”‚    â””â”€â”€ annotations/\n",
        "   â”œâ”€â”€ val/ (new folder)\n",
        "   â”‚    â”œâ”€â”€ images/\n",
        "   â”‚    â””â”€â”€ annotations/\n",
        "   â””â”€â”€ test/ (new folder)\n",
        "        â”œâ”€â”€ images/\n",
        "        â””â”€â”€ annotations/\n",
        "```"
      ],
      "id": "x8tiiYhvmqa-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jBPs2k8mqa-",
        "outputId": "a7bb5bec-fee7-44ac-e3e2-e1b0c1a1256b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os, shutil, random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define the original dataset directories\n",
        "base_dir = '/content/HardHat_Dataset_1k'\n",
        "orig_images_dir = os.path.join(base_dir, 'images')\n",
        "orig_ann_dir = os.path.join(base_dir, 'annotations')\n",
        "\n",
        "# Create new split directories\n",
        "splits = ['train', 'val', 'test']\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_dir, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, split, 'annotations'), exist_ok=True)\n",
        "\n",
        "# Get list of all image files\n",
        "all_images = [f for f in os.listdir(orig_images_dir) if f.endswith('.png')]\n",
        "total_images = len(all_images)\n",
        "print(f\"Total images found: {total_images}\")\n",
        "\n",
        "# Shuffle and split into 80/10/10\n",
        "random.shuffle(all_images)\n",
        "train_end = int(0.8 * total_images)\n",
        "val_end = int(0.9 * total_images)\n",
        "\n",
        "train_imgs = all_images[:train_end]\n",
        "val_imgs = all_images[train_end:val_end]\n",
        "test_imgs = all_images[val_end:]\n",
        "\n",
        "print(f\"Train: {len(train_imgs)}, Validation: {len(val_imgs)}, Test: {len(test_imgs)}\")\n",
        "\n",
        "def copy_files(file_list, src_images, src_ann, dest_split):\n",
        "    dest_img = os.path.join(base_dir, dest_split, 'images')\n",
        "    dest_ann = os.path.join(base_dir, dest_split, 'annotations')\n",
        "    for fname in file_list:\n",
        "        shutil.copy(os.path.join(src_images, fname), dest_img)\n",
        "        ann_fname = os.path.splitext(fname)[0] + '.xml'\n",
        "        src_ann_file = os.path.join(src_ann, ann_fname)\n",
        "        if os.path.exists(src_ann_file):\n",
        "            shutil.copy(src_ann_file, dest_ann)\n",
        "\n",
        "copy_files(train_imgs, orig_images_dir, orig_ann_dir, 'train')\n",
        "copy_files(val_imgs, orig_images_dir, orig_ann_dir, 'val')\n",
        "copy_files(test_imgs, orig_images_dir, orig_ann_dir, 'test')\n",
        "\n",
        "print(\"Dataset split into train, validation, and test sets.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 1000\n",
            "Train: 800, Validation: 100, Test: 100\n",
            "Dataset split into train, validation, and test sets.\n"
          ]
        }
      ],
      "id": "5jBPs2k8mqa-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAF2ICGKmqa-"
      },
      "source": [
        "## 3. YOLOv5 Dataset Configuration\n",
        "\n",
        "Create a YAML configuration file (named `hardhat.yaml`) for YOLOv5. It should point to the train and validation image directories and list the class names."
      ],
      "id": "zAF2ICGKmqa-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzvPys8Umqa-",
        "outputId": "14d23d77-8c56-42b7-f49b-4cd4d9062ff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import yaml\n",
        "\n",
        "config_data = {\n",
        "    'train': os.path.join(base_dir, 'train', 'images'),\n",
        "    'val': os.path.join(base_dir, 'val', 'images'),\n",
        "    'names': ['helmet', 'head'],\n",
        "    'nc': 2\n",
        "}\n",
        "\n",
        "with open('hardhat.yaml', 'w') as f:\n",
        "    yaml.dump(config_data, f)\n",
        "\n",
        "print(\"Created YOLOv5 dataset configuration file: hardhat.yaml\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created YOLOv5 dataset configuration file: hardhat.yaml\n"
          ]
        }
      ],
      "id": "zzvPys8Umqa-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VLlij1vmqa-"
      },
      "source": [
        "## 4. Setting Up YOLOv5 Environment\n",
        "\n",
        "If you haven't already cloned YOLOv5, do so now. This code is intended to run in Google Colab."
      ],
      "id": "-VLlij1vmqa-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQRr8ugkmqa_",
        "outputId": "a275864a-7693-4774-d36e-d8147eb68329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(\"Running in Colab?\", IN_COLAB)\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/ultralytics/yolov5.git\n",
        "    %cd yolov5\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"Ensure YOLOv5 is installed locally.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab? True\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17270, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 17270 (delta 0), reused 0 (delta 0), pack-reused 17269 (from 2)\u001b[K\n",
            "Receiving objects: 100% (17270/17270), 16.12 MiB | 25.54 MiB/s, done.\n",
            "Resolving deltas: 100% (11858/11858), done.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n",
            "  Downloading ultralytics-8.3.75-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.75-py3-none-any.whl (914 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238 ultralytics-8.3.75 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "id": "pQRr8ugkmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnRqD1-vmqa_"
      },
      "source": [
        "## 5. Training YOLOv5 on the Dataset\n",
        "\n",
        "Now that the dataset is split and the configuration file is ready, we can start training.\n",
        "\n",
        "This example uses the pretrained YOLOv5s model and fine-tunes it for 10 epochs. You can monitor training progress (loss, mAP, etc.) via the training logs and by checking the generated `results.png` in the output folder."
      ],
      "id": "tnRqD1-vmqa_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES57wChcmqa_",
        "outputId": "53046592-cb47-4b89-e603-a9de07001f67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    !python train.py --img 640 --batch 16 --epochs 10 --data ../hardhat.yaml --weights yolov5s.pt --name yolo_hardhat_exp\n",
        "else:\n",
        "    print(\"Run the YOLOv5 training command in your local environment.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2025-02-14 18:37:36.696574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739558256.962827    2448 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739558257.045286    2448 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../hardhat.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolo_hardhat_exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ğŸš€ v7.0-398-g5cdad892 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ğŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 23.8MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 192MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "/content/yolov5/models/common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "/content/yolov5/models/common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0m6 validation errors for InitSchema\n",
            "p\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "scale\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "ratio\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "size\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "interpolation\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "mask_interpolation\n",
            "  Field required [type=missing, input_value={}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/HardHat_Dataset_1k/train/labels... 0 images, 800 backgrounds, 0 corrupt: 100% 800/800 [00:02<00:00, 364.73it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ No labels found in /content/HardHat_Dataset_1k/train/labels.cache. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/HardHat_Dataset_1k/train/labels.cache\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5/train.py\", line 986, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5/train.py\", line 688, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov5/train.py\", line 285, in train\n",
            "    train_loader, dataset = create_dataloader(\n",
            "                            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 184, in create_dataloader\n",
            "    dataset = LoadImagesAndLabels(\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 611, in __init__\n",
            "    assert nf > 0 or not augment, f\"{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /content/HardHat_Dataset_1k/train/labels.cache, can not start training. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n"
          ]
        }
      ],
      "id": "ES57wChcmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMwZQZNOmqa_"
      },
      "source": [
        "## 6. Monitoring Training & Model Testing\n",
        "\n",
        "During training, YOLOv5 outputs logs that include training/validation loss and mAP metrics. After training, review the `runs/train/yolo_hardhat_exp/results.png` plot to check the performance curves.\n",
        "\n",
        "After training, test the model on the test set. Use the following command to run inference on test images."
      ],
      "id": "NMwZQZNOmqa_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgIIF1Cimqa_"
      },
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    !python detect.py --weights runs/train/yolo_hardhat_exp/weights/best.pt --img 640 --conf 0.25 --source ../HardHat_Dataset/test/images --name test_inference\n",
        "else:\n",
        "    print(\"Run the detection command in your local environment.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "JgIIF1Cimqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzCNh1zOmqa_"
      },
      "source": [
        "## 7. Visualizing Inference Results\n",
        "\n",
        "After running inference, check the folder `runs/detect/test_inference` for images with predicted bounding boxes. Display a sample result image below:"
      ],
      "id": "hzCNh1zOmqa_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnIpNhS8mqa_"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Set path to a sample result image (adjust filename if necessary)\n",
        "result_img = '/content/yolov5/runs/detect/test_inference/exp/image_001.jpg'\n",
        "if os.path.exists(result_img):\n",
        "    display(Image(filename=result_img))\n",
        "else:\n",
        "    print(\"Result image not found. Check your detection output folder.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "SnIpNhS8mqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q9y0Zkrmqa_"
      },
      "source": [
        "## 8. Tips for Improvement & Next Steps\n",
        "\n",
        "1. **Data Augmentation**: Increase training diversity with flips, rotations, or color jitter using libraries like Albumentations.\n",
        "2. **Longer Training**: Increase epochs if you have sufficient GPU resources.\n",
        "3. **Hyperparameter Tuning**: Experiment with batch size, learning rate, and other YOLOv5 hyperparameters.\n",
        "4. **Real-World Deployment**: Explore integrating your model with live camera feeds for real-time safety monitoring on construction sites.\n",
        "5. **Model Evaluation**: Continuously monitor metrics such as mAP, precision, and recall to improve model performance.\n"
      ],
      "id": "2Q9y0Zkrmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG4lQrBDmqa_"
      },
      "source": [
        "## 9. Conclusion & Next Steps\n",
        "\n",
        "In this notebook, you learned how to:\n",
        "1. Download a Hardhat Detection dataset from a shared Google Drive link using `gdown`.\n",
        "2. Split the dataset into training, validation, and testing sets.\n",
        "3. Configure YOLOv5 and start training with monitoring.\n",
        "4. Run inference and test the model on unseen data.\n",
        "\n",
        "Deep learning methods like YOLOv5 can greatly enhance safety monitoring on construction sites. Continue experimenting with data augmentation, hyperparameter tuning, and integration with real-time systems.\n",
        "\n",
        "Happy Coding and Stay Safe on Site!"
      ],
      "id": "WG4lQrBDmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKpHgK64mqa_"
      },
      "source": [
        "---\n",
        "# Resources & References\n",
        "1. [YOLOv5 GitHub](https://github.com/ultralytics/yolov5)\n",
        "2. [Hardhat Detection Dataset on GitHub](<YOUR_GITHUB_REPO_URL>)\n",
        "3. [Ultralytics YOLOv5 Documentation](https://docs.ultralytics.com/)\n",
        "4. [Albumentations Library](https://github.com/albumentations-team/albumentations)\n",
        "\n",
        "Feel free to modify paths, hyperparameters, or configurations as needed."
      ],
      "id": "yKpHgK64mqa_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}