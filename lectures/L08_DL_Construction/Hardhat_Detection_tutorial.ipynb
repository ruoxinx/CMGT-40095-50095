{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8rVA-y1mqa7"
      },
      "source": [
        "# Safety Hardhat Detection in Construction: Deep Learning Tutorial\n",
        "**Author:** Ruoxin Xiong, Ph.D., Assistant Professor \\\\\n",
        "**Affiliation:** Construction Management, College of Architecture and Environmental Design, Kent State University\n",
        "\n",
        "## Overview\n",
        "In this notebook, you will:\n",
        "1. Introduce the *Hard Hat Detection* dataset from Kaggle.\n",
        "2. Load and split the dataset into training, validation, and testing sets.\n",
        "3. Set up a [**YOLOv5** (You Only Look Once, version 5)](https://docs.ultralytics.com/) environment to train an object detection model on the dataset.\n",
        "4. Evaluate the model's performance using common metrics (mAP, precision, recall).\n",
        "5. Provide tips for next steps and improvements.\n",
        "\n",
        "> **Note:** This notebook is designed for Google Colab with GPU enabled. \\\n",
        " Make sure to **enable GPU** under `*Runtime* > *Change runtime type* > *Hardware Accelerator* = GPU`."
      ],
      "id": "B8rVA-y1mqa7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Safety Hardhat Detection in Construction**\n",
        "\n",
        "### Why Hardhat Detection?\n",
        "- Construction sites can be **dangerous** if workers do not follow proper safety protocols (like wearing a helmet / hardhat).\n",
        "- Automated **computer vision** can detect if workers in images or video feeds are wearing helmets.\n",
        "- Useful for **real-time safety monitoring**, compliance reporting, or risk management.\n",
        "\n",
        "### About the Dataset\n",
        "- *Hard Hat Detection Dataset* ([link](https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection)) has ~10,000 images.\n",
        "- Annotations include bounding boxes for:\n",
        "  1. `helmet`\n",
        "  2. `head` (no helmet)\n",
        "- We'll focus on detecting whether a worker's head is protected by a helmet or not.\n",
        "\n",
        "### Tools We'll Use\n",
        "1. **Python** + **pandas**, **matplotlib** for data handling and visualization.\n",
        "2. **YOLOv5** for object detection:\n",
        "   - YOLO is a popular, state-of-the-art object detection approach.\n",
        "   - We can quickly train a custom model on new data.\n",
        "3. **Google Colab** or local GPU environment for faster training (recommended, but you can do CPU-only with slower training)."
      ],
      "metadata": {
        "id": "pEFi7bel7MyA"
      },
      "id": "pEFi7bel7MyA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXKplBVZmqa8"
      },
      "source": [
        "## 1. Downloading the Dataset\n",
        "\n",
        "We use `gdown` to download the zipped dataset from a shared Google Drive link. Replace `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your file's ID. The expected folder structure after unzipping is:\n",
        "\n",
        "```\n",
        "HardHat_Dataset/\n",
        "   ├── images/\n",
        "   │    ├── image_0001.png # All image files (.png)\n",
        "   │    ├── ...\n",
        "   └── annotations/\n",
        "        ├── image_0001.xml # Annotation files in VOC XML format\n",
        "        ├── ...\n",
        "```"
      ],
      "id": "aXKplBVZmqa8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlAaVUEymqa9"
      },
      "source": [
        "# Install gdown if not already installed\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ZlAaVUEymqa9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-SEqfBFmqa9"
      },
      "source": [
        "### Download the Dataset from Google Drive\n",
        "Replace `<YOUR_GOOGLE_DRIVE_FILE_ID>` with your shared file ID. This file should be a zip archive of the dataset.\n",
        "\n",
        "- **Full-size dataset** (`HardHat_Dataset`) contains ~10,000 images for model training and testing. Link: https://drive.google.com/file/d/1ZDGJ3tWMqRAdbHviFYXHyvjJQ88tL_c5/view\n",
        "- **Small-size dataset** (`HardHat_Dataset_1k`) contains ~1,000 images randomly sampled from the full dataset. Link: https://drive.google.com/file/d/1XebIf0c3LDe_KNcPR6KI8Ys4ReNj6knG/view"
      ],
      "id": "l-SEqfBFmqa9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saIaG3sumqa-"
      },
      "source": [
        "import gdown\n",
        "\n",
        "# If you are using full datset, please replace with your file ID from the shared Google Drive link\n",
        "file_id = '1XebIf0c3LDe_KNcPR6KI8Ys4ReNj6knG'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = '/content/hardhat_dataset_1k.zip'\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q /content/hardhat_dataset_1k.zip -d /content/HardHat_Dataset_1k\n",
        "print(\"Dataset downloaded and unzipped to /content/HardHat_Dataset_1k\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "saIaG3sumqa-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8tiiYhvmqa-"
      },
      "source": [
        "## 2. Split the Dataset into Train, Validation, and Test Sets\n",
        "\n",
        "The raw dataset does not have predefined splits. We'll create three subsets (80% train, 10% validation, 10% test). Additionally, because YOLOv5 expects annotation files in a folder named `labels` (in YOLO format), we'll convert the XML annotations (VOC format) to YOLO format during the splitting process.\n",
        "\n",
        "The expected folder structure after splitting:\n",
        "\n",
        "```\n",
        "HardHat_Dataset/\n",
        "   ├── images/           (raw images)\n",
        "   ├── annotations/      (raw XML annotations)\n",
        "   ├── train/\n",
        "   │    ├── images/\n",
        "   │    └── labels/     (converted YOLO annotations)\n",
        "   ├── val/\n",
        "   │    ├── images/\n",
        "   │    └── labels/\n",
        "   └── test/\n",
        "        ├── images/\n",
        "        └── labels/\n",
        "```"
      ],
      "id": "x8tiiYhvmqa-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jBPs2k8mqa-"
      },
      "source": [
        "import os, shutil, random\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Define raw dataset directories\n",
        "base_dir = '/content/HardHat_Dataset_1k'\n",
        "orig_images_dir = os.path.join(base_dir, 'images')\n",
        "orig_ann_dir = os.path.join(base_dir, 'annotations')\n",
        "\n",
        "# Create new split directories for train, val, test (with 'images' and 'labels' folders)\n",
        "splits = ['train', 'val', 'test']\n",
        "for split in splits:\n",
        "    os.makedirs(os.path.join(base_dir, split, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(base_dir, split, 'labels'), exist_ok=True)\n",
        "\n",
        "# Get list of all image files\n",
        "all_images = [f for f in os.listdir(orig_images_dir) if f.endswith('.png')]\n",
        "total_images = len(all_images)\n",
        "print(f\"Total images found: {total_images}\")\n",
        "\n",
        "# Shuffle and split images into 80% train, 10% val, 10% test\n",
        "random.shuffle(all_images)\n",
        "train_end = int(0.8 * total_images)\n",
        "val_end = int(0.9 * total_images)\n",
        "\n",
        "train_imgs = all_images[:train_end]\n",
        "val_imgs = all_images[train_end:val_end]\n",
        "test_imgs = all_images[val_end:]\n",
        "\n",
        "print(f\"Train: {len(train_imgs)}, Validation: {len(val_imgs)}, Test: {len(test_imgs)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5jBPs2k8mqa-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classes for YOLO conversion (adjust if needed)\n",
        "classes = ['helmet', 'head']\n",
        "\n",
        "def convert_xml_to_yolo(xml_file, classes):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    size = root.find('size')\n",
        "    width = float(size.find('width').text)\n",
        "    height = float(size.find('height').text)\n",
        "    yolo_lines = []\n",
        "    for obj in root.findall('object'):\n",
        "        cls = obj.find('name').text\n",
        "        if cls not in classes:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        bndbox = obj.find('bndbox')\n",
        "        xmin = float(bndbox.find('xmin').text)\n",
        "        ymin = float(bndbox.find('ymin').text)\n",
        "        xmax = float(bndbox.find('xmax').text)\n",
        "        ymax = float(bndbox.find('ymax').text)\n",
        "        x_center = ((xmin + xmax) / 2) / width\n",
        "        y_center = ((ymin + ymax) / 2) / height\n",
        "        bbox_width = (xmax - xmin) / width\n",
        "        bbox_height = (ymax - ymin) / height\n",
        "        yolo_lines.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\")\n",
        "    return yolo_lines\n",
        "\n",
        "def copy_and_convert_files(file_list, src_images, src_ann, dest_split):\n",
        "    dest_img = os.path.join(base_dir, dest_split, 'images')\n",
        "    dest_lbl = os.path.join(base_dir, dest_split, 'labels')\n",
        "    for fname in file_list:\n",
        "        # Copy image file\n",
        "        shutil.copy(os.path.join(src_images, fname), dest_img)\n",
        "\n",
        "        base_name = os.path.splitext(fname)[0]\n",
        "        xml_file = os.path.join(src_ann, base_name + '.xml')\n",
        "        txt_file = os.path.join(src_ann, base_name + '.txt')\n",
        "\n",
        "        # If annotation is in XML, convert it; otherwise copy txt\n",
        "        if os.path.exists(xml_file):\n",
        "            yolo_lines = convert_xml_to_yolo(xml_file, classes)\n",
        "            if yolo_lines:\n",
        "                with open(os.path.join(dest_lbl, base_name + '.txt'), 'w') as f:\n",
        "                    f.write(\"\\n\".join(yolo_lines))\n",
        "        elif os.path.exists(txt_file):\n",
        "            shutil.copy(txt_file, dest_lbl)\n",
        "        else:\n",
        "            print(f\"Warning: No annotation found for {fname}\")\n",
        "\n",
        "# Copy and convert annotations for each split\n",
        "copy_and_convert_files(train_imgs, orig_images_dir, orig_ann_dir, 'train')\n",
        "copy_and_convert_files(val_imgs, orig_images_dir, orig_ann_dir, 'val')\n",
        "copy_and_convert_files(test_imgs, orig_images_dir, orig_ann_dir, 'test')\n",
        "\n",
        "print(\"Dataset split and annotations converted (if needed) into train, validation, and test sets.\")"
      ],
      "metadata": {
        "id": "9X2-FYuf9JrL"
      },
      "id": "9X2-FYuf9JrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Verify splits\n",
        "for split in splits:\n",
        "    images_count = len(os.listdir(os.path.join(base_dir, split, 'images')))\n",
        "    labels_count = len(os.listdir(os.path.join(base_dir, split, 'labels')))\n",
        "    print(f\"{split.capitalize()} - Images: {images_count}, Labels: {labels_count}\")"
      ],
      "metadata": {
        "id": "lwxrjvPr9MhE"
      },
      "id": "lwxrjvPr9MhE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Basic Data Exploration & Visualization\n",
        "Let's do a quick check on the **number of images** and show some **sample bounding boxes**."
      ],
      "metadata": {
        "id": "X_9zCZaO9jgO"
      },
      "id": "X_9zCZaO9jgO"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "image_files = glob.glob(os.path.join(base_dir, 'images', '*.png'))\n",
        "print(\"Total Images:\", len(image_files))\n",
        "\n",
        "# Display a random sample image (without bounding boxes for now)\n",
        "import random\n",
        "seed = 0\n",
        "\n",
        "sample_img = random.choice(image_files)\n",
        "img_bgr = cv2.imread(sample_img)  # BGR format\n",
        "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(img_rgb)\n",
        "plt.title(os.path.basename(sample_img))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UL0LazuM9q7E"
      },
      "id": "UL0LazuM9q7E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display bounding boxes for the random image"
      ],
      "metadata": {
        "id": "jUCb9G85rc7P"
      },
      "id": "jUCb9G85rc7P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to parse VOC XML and return bounding boxes and class names\n",
        "def parse_voc_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    boxes = []\n",
        "    for obj in root.findall('object'):\n",
        "        cls = obj.find('name').text\n",
        "        bndbox = obj.find('bndbox')\n",
        "        xmin = int(float(bndbox.find('xmin').text))\n",
        "        ymin = int(float(bndbox.find('ymin').text))\n",
        "        xmax = int(float(bndbox.find('xmax').text))\n",
        "        ymax = int(float(bndbox.find('ymax').text))\n",
        "        boxes.append((cls, xmin, ymin, xmax, ymax))\n",
        "    return boxes\n",
        "\n",
        "# Get the corresponding annotation file (assumes same basename with .xml extension)\n",
        "base_name = os.path.splitext(os.path.basename(sample_img))[0]\n",
        "xml_file = os.path.join(base_dir, 'annotations', base_name + '.xml')\n",
        "\n",
        "# Parse XML annotations if available\n",
        "if os.path.exists(xml_file):\n",
        "    boxes = parse_voc_xml(xml_file)\n",
        "else:\n",
        "    boxes = []\n",
        "    print(\"Annotation XML not found for\", sample_img)\n",
        "\n",
        "# Draw bounding boxes on the image\n",
        "for (cls, xmin, ymin, xmax, ymax) in boxes:\n",
        "    # Draw rectangle: color (0, 255, 0) is green and thickness 2\n",
        "    cv2.rectangle(img_rgb, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "    # Put class text on top-left corner of the bounding box\n",
        "    cv2.putText(img_rgb, cls, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9, (0, 255, 0), 2)\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(img_rgb)\n",
        "plt.title(os.path.basename(sample_img))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t_CO8ptUq1Xj"
      },
      "id": "t_CO8ptUq1Xj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAF2ICGKmqa-"
      },
      "source": [
        "## 4. YOLOv5 Dataset Configuration\n",
        "\n",
        "Create a YAML configuration file (named `hardhat.yaml`) for YOLOv5.\n",
        "\n",
        "YOLOv5 expects a directory structure like:\n",
        "```\n",
        "yolov5/\n",
        "   ├── data/\n",
        "   │    └── your_dataset.yaml   (dataset config)\n",
        "   ├── dataset images\n",
        "   └── dataset labels\n",
        "```\n",
        "We'll create a **configuration file** that points to your train/test images and the class names. For example:\n",
        "```\n",
        "hardhat.yaml:\n",
        "train: /content/HardHat_Dataset_1k/train/images\n",
        "val: /content/HardHat_Dataset_1k/val/images\n",
        "test: /content/HardHat_Dataset_1k/test/images  # optional\n",
        "\n",
        "names: [\"helmet\", \"head\"]  # or your set of classes\n",
        "nc: 2  # number of classes\n",
        "```"
      ],
      "id": "zAF2ICGKmqa-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzvPys8Umqa-"
      },
      "source": [
        "import yaml\n",
        "\n",
        "config_data = {\n",
        "    'train': os.path.join(base_dir, 'train', 'images'),\n",
        "    'val': os.path.join(base_dir, 'val', 'images'),\n",
        "    'names': ['helmet', 'head'],\n",
        "    'nc': 2\n",
        "}\n",
        "\n",
        "with open('hardhat.yaml', 'w') as f:\n",
        "    yaml.dump(config_data, f)\n",
        "\n",
        "print(\"Created YOLOv5 dataset configuration file: hardhat.yaml\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "zzvPys8Umqa-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VLlij1vmqa-"
      },
      "source": [
        "## 5. Setting Up YOLOv5 Environment\n",
        "\n",
        "If you haven't already cloned YOLOv5, do so now. This code is intended to run in Google Colab."
      ],
      "id": "-VLlij1vmqa-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQRr8ugkmqa_"
      },
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(\"Running in Colab?\", IN_COLAB)\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/ultralytics/yolov5.git\n",
        "    %cd yolov5\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"Ensure YOLOv5 is installed locally.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "pQRr8ugkmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnRqD1-vmqa_"
      },
      "source": [
        "## 6. Training YOLOv5 on the Dataset\n",
        "\n",
        "Now that the dataset is split and the configuration file is ready, we can start training.\n",
        "\n",
        "From within the `yolov5/` directory, you can run:\n",
        "```\n",
        "!python train.py --img 640 --batch 16 --epochs 30 \\\n",
        "  --data hardhat.yaml --weights yolov5s.pt \\\n",
        "  --name yolo_hardhat_exp\n",
        "```"
      ],
      "id": "tnRqD1-vmqa_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining each argument:\n",
        "- **--img 640**: The image resolution for training.\n",
        "- **--batch 16**: Batch size (adjust based on GPU memory).\n",
        "- **--epochs 10**: Number of training epochs. Increase if you have enough time and data.\n",
        "- **--data hardhat.yaml**: Path to your dataset config file.\n",
        "- **--weights yolov5s.pt**: Starting from a pretrained YOLOv5 model.\n",
        "- **--name yolo_hardhat_exp**: Output folder name for results."
      ],
      "metadata": {
        "id": "vVjK9QVB_20V"
      },
      "id": "vVjK9QVB_20V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, YOLOv5 will display training/validation losses, mAP (mean Average Precision), and more.\n",
        "\n",
        "- **mAP@0.5**: The primary object detection metric. Closer to 1 means better.\n",
        "- **Precision / Recall**: Also measured for each class. Good to see whether the model is catching heads with and without helmets.\n",
        "\n",
        "During training, YOLOv5 logs metrics per epoch. After training finishes, you can look at `runs/train/yolo_hardhat_exp` for:\n",
        "- `results.png` plot of training/validation curves.\n",
        "- Best weights stored as `best.pt`."
      ],
      "metadata": {
        "id": "sMERCP5C_23w"
      },
      "id": "sMERCP5C_23w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES57wChcmqa_"
      },
      "source": [
        "if IN_COLAB:\n",
        "    %cd /content/yolov5\n",
        "    !python train.py --img 640 --batch 16 --epochs 10 --data ../hardhat.yaml --weights yolov5s.pt --name yolo_hardhat_exp\n",
        "else:\n",
        "    print(\"Run the YOLOv5 training command in your local environment.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ES57wChcmqa_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Testing & Performance Evaluation on Test Subset\n",
        "Use your trained model to predict on **unseen** images:\n",
        "```\n",
        "!python detect.py --weights runs/train/yolo_hardhat_exp/weights/best.pt \\\n",
        "                  --img 640 --conf 0.25 --source /content/HardHat_Dataset-1k/test/images\n",
        "```\n",
        "This will create bounding box predictions in `runs/detect/`.\n"
      ],
      "metadata": {
        "id": "EA8_CrRFBhoM"
      },
      "id": "EA8_CrRFBhoM"
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "    %cd /content/yolov5\n",
        "    print(\"Running inference on test set with --save-txt...\")\n",
        "    !python detect.py --weights runs/train/yolo_hardhat_exp/weights/best.pt --img 640 --conf 0.25 --save-txt --source ../HardHat_Dataset_1k/test/images --name test_inference\n",
        "else:\n",
        "    print(\"Run the detection command in your local environment with --save-txt.\")"
      ],
      "metadata": {
        "id": "Q_wdgwGQINWE"
      },
      "id": "Q_wdgwGQINWE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzCNh1zOmqa_"
      },
      "source": [
        "## 8. Visualizing Inference Results\n",
        "\n",
        "After running inference, check the folder `runs/detect/test_inference`  folder for images with bounding boxes over `helmet` or `head`.\n",
        "A typical bounding box label might read `helmet 0.91`, indicating the model is **91% confident** it's a helmet."
      ],
      "id": "hzCNh1zOmqa_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnIpNhS8mqa_"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Set path to a sample result image (adjust filename if necessary)\n",
        "result_img = '/content/yolov5/runs/detect/test_inference/hard_hat_workers1046.png'\n",
        "if os.path.exists(result_img):\n",
        "    display(Image(filename=result_img))\n",
        "else:\n",
        "    print(\"Result image not found. Check your detection output folder.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "SnIpNhS8mqa_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display 5 detected result images with bounding boxes"
      ],
      "metadata": {
        "id": "cMtj3y0AqAFx"
      },
      "id": "cMtj3y0AqAFx"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Define the directory where YOLOv5 saves detected result images (with bounding boxes)\n",
        "# This directory is created by the detect.py script when run with the --save-txt flag.\n",
        "detected_results_dir = '/content/yolov5/runs/detect/test_inference'\n",
        "\n",
        "# Gather list of detected result image files\n",
        "detected_image_files = glob.glob(f\"{detected_results_dir}/*.png\")\n",
        "print(f\"Found {len(detected_image_files)} detected result images.\")\n",
        "\n",
        "# Display 5 detected result images with bounding boxes\n",
        "for img_path in detected_image_files[:5]:\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Detected Result: {os.path.basename(img_path)}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Zn1rAs4Zi8O3"
      },
      "id": "Zn1rAs4Zi8O3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q9y0Zkrmqa_"
      },
      "source": [
        "## 9. Tips for Improvement & Next Steps\n",
        "\n",
        "- **Edge Deployment**: Running these models on site (in real-time) may require smaller, faster models or specialized hardware.\n",
        "- **False Positives/Negatives**: A missed detection of a worker without a helmet can have safety implications.\n",
        "- **Privacy & Ethics**: Worker monitoring must follow local regulations and respect privacy.\n",
        "- **Integration**: Alerts or logs can integrate with a construction management system.\n"
      ],
      "id": "2Q9y0Zkrmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG4lQrBDmqa_"
      },
      "source": [
        "## 10. Conclusion & Next Steps\n",
        "\n",
        "In this notebook, you've:\n",
        "1. Explored how to set up YOLOv5 for **hardhat detection**.\n",
        "2. Learned basic steps of data preparation, training, and inference.\n",
        "3. Seen how to interpret object detection metrics (mAP, precision, recall).\n",
        "\n",
        "**Next Steps**:\n",
        "- Expand your dataset or gather your own site images.\n",
        "- Tune hyperparameters, try advanced YOLO versions or other detection frameworks.\n",
        "- Explore **live camera feed** integration if you want real-time detection on a construction site.\n",
        "- Keep refining the model, especially for edge cases (nighttime, partial occlusions, reflective surfaces).\n",
        "\n",
        "Deep learning can **dramatically** improve safety monitoring and compliance tracking for construction teams. Continue learning, stay curious, and best of luck in building a safer job site with AI!"
      ],
      "id": "WG4lQrBDmqa_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKpHgK64mqa_"
      },
      "source": [
        "---\n",
        "# **Resources & References**\n",
        "1. [YOLOv5 GitHub](https://github.com/ultralytics/yolov5)\n",
        "2. [Kaggle Hard Hat Detection](https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection)\n",
        "3. [Ultralytics Documentation](https://docs.ultralytics.com/) for YOLOv5 usage.\n",
        "4. [Albumentations Library](https://github.com/albumentations-team/albumentations) for data augmentation.\n",
        "5. Additional frameworks: [Detectron2 (Facebook AI)](https://github.com/facebookresearch/detectron2), [MMDetection](https://github.com/open-mmlab/mmdetection).\n",
        "\n",
        "Feel free to modify paths, hyperparameters, or configurations as needed."
      ],
      "id": "yKpHgK64mqa_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84xrzQGGVVdz"
      },
      "id": "84xrzQGGVVdz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}