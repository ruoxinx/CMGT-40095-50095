{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hardhat Detection in Construction: A Beginner's Deep Learning Tutorial**\n",
    "\n",
    "Author: *[Your Name]*  \n",
    "Date: *[Today's Date]*\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll:\n",
    "1. Introduce the *Hard Hat Detection* dataset from Kaggle.\n",
    "2. Load and visualize sample images with bounding box labels.\n",
    "3. Set up a **YOLOv5** (You Only Look Once, version 5) or a **Detectron2** environment to train an object detection model on the dataset.\n",
    "4. Evaluate the model's performance using common metrics (mAP, precision, recall).\n",
    "5. Provide tips for next steps and improvements.\n",
    "\n",
    "> **Note**: You do *not* need prior deep learning experience. We'll explain each step with clarity.\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Hardhat Detection in Construction\n",
    "\n",
    "### Why Hardhat Detection?\n",
    "- Construction sites can be **dangerous** if workers do not follow proper safety protocols (like wearing a helmet / hardhat).\n",
    "- Automated **computer vision** can detect if workers in images or video feeds are wearing helmets.\n",
    "- Useful for **real-time safety monitoring**, compliance reporting, or risk management.\n",
    "\n",
    "### About the Dataset\n",
    "- *Kaggle Hard Hat Detection* ([link](https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection)) has ~1,100 images.\n",
    "- Annotations include bounding boxes for:\n",
    "  1. `person` (worker)\n",
    "  2. `helmet`\n",
    "  3. `head` (no helmet)\n",
    "- We'll focus on detecting whether a worker's head is protected by a helmet or not.\n",
    "\n",
    "### Tools We'll Use\n",
    "1. **Python** + **pandas**, **matplotlib** for data handling and visualization.\n",
    "2. **YOLOv5** for object detection:\n",
    "   - YOLO is a popular, state-of-the-art object detection approach.\n",
    "   - We can quickly train a custom model on new data.\n",
    "3. **Google Colab** or local GPU environment for faster training (recommended, but you can do CPU-only with slower training).\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "We'll install/clone YOLOv5, then confirm that dependencies are in place.\n",
    "\n",
    "> If you're on **Google Colab**:\n",
    "1. Upload this notebook (`Hardhat_Detection.ipynb`).\n",
    "2. Make sure to **enable GPU** under *Runtime* > *Change runtime type* > *Hardware Accelerator* = GPU.\n"
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SYSTEM CHECK: Are we running in Colab?\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(\"Running in Colab?\", IN_COLAB)\n",
    "\n",
    "# If in Colab, clone YOLOv5 and install dependencies.\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/ultralytics/yolov5.git\n",
    "    %cd yolov5\n",
    "    !pip install -r requirements.txt  # install required libraries\n",
    "else:\n",
    "    print(\"Please make sure you've cloned YOLOv5 or installed it locally.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download the Dataset\n",
    "\n",
    "### Kaggle Hard Hat Detection\n",
    "- We'll assume you've downloaded the Kaggle dataset zip file (`hard-hat-detection.zip`) containing images and annotation files.\n",
    "- If you're in Colab, you could upload it to your Drive and then mount your drive, or use the `kaggle` API.\n",
    "\n",
    "> For simplicity, let's assume you have the dataset folder structure like this:\n",
    "```\n",
    "HardHat_Dataset/\n",
    "  ├── images/\n",
    "  │   ├── image_0001.jpg\n",
    "  │   ├── ...\n",
    "  └── annotations/\n",
    "      ├── image_0001.txt  (YOLO or COCO format bounding boxes)\n",
    "      ├── ...\n",
    "```\n",
    "We'll convert or adapt to YOLO-friendly format if needed."
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example code to unzip dataset in Colab or local environment.\n",
    "# NOTE: Adjust the paths to match your environment.\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_zip = '/content/hard-hat-detection.zip'  # CHANGE THIS PATH!\n",
    "dataset_dir = '/content/HardHat_Dataset'         # Desired extract location\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Example: unzipping in Colab\n",
    "    if os.path.exists(dataset_zip):\n",
    "        !unzip -q {dataset_zip} -d {dataset_dir}\n",
    "        print(\"Dataset unzipped successfully!\")\n",
    "    else:\n",
    "        print(\"Dataset zip not found.\")\n",
    "else:\n",
    "    print(\"Please unzip your dataset locally or specify paths correctly.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Data Exploration & Visualization\n",
    "Let's do a quick check on the **number of images** and show some **sample bounding boxes**. If your dataset is already in YOLO format, you'll have `.txt` annotation files for each image. Otherwise, you might have JSON or XML (VOC, COCO) that you need to convert.\n"
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "image_files = glob.glob(os.path.join(dataset_dir, 'images', '*.jpg'))\n",
    "print(\"Total Images:\", len(image_files))\n",
    "\n",
    "# Display a random sample image (without bounding boxes for now)\n",
    "import random\n",
    "\n",
    "sample_img = random.choice(image_files)\n",
    "img_bgr = cv2.imread(sample_img)  # BGR format\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(os.path.basename(sample_img))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Annotations (Optional Preview)\n",
    "If your bounding boxes are in YOLO txt format (class, x_center, y_center, width, height) scaled to [0,1], you can draw them to ensure the labeling is correct.\n",
    "\n",
    "> We'll skip a detailed code snippet here to keep it simpler, but you can parse the `.txt` file, extract bounding boxes, and draw them on the image for a random sample.\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting Up YOLOv5 Training\n",
    "\n",
    "YOLOv5 expects a directory structure like:\n",
    "```\n",
    "yolov5/\n",
    "   ├── data/\n",
    "   │    └── your_dataset.yaml   (dataset config)\n",
    "   ├── dataset images\n",
    "   └── dataset labels\n",
    "```\n",
    "We'll create a **configuration file** that points to your train/test images and the class names. For example:\n",
    "```\n",
    "hardhat.yaml:\n",
    "train: /content/HardHat_Dataset/train/images\n",
    "val: /content/HardHat_Dataset/val/images\n",
    "test: /content/HardHat_Dataset/test/images  # optional\n",
    "\n",
    "names: [\"helmet\", \"head\"]  # or your set of classes\n",
    "nc: 2  # number of classes\n",
    "```\n",
    "You can split the dataset into train/val/test subsets (e.g. 80/10/10) either manually or using a script.\n"
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# EXAMPLE: We won't run this if we don't have the data splits.\n",
    "# But let's pretend we have created a config file.\n",
    "import yaml\n",
    "\n",
    "config_data = {\n",
    "    'train': '/content/HardHat_Dataset/train/images',\n",
    "    'val': '/content/HardHat_Dataset/val/images',\n",
    "    'names': ['helmet', 'head'],\n",
    "    'nc': 2\n",
    "}\n",
    "\n",
    "with open('hardhat.yaml', 'w') as f:\n",
    "    yaml.dump(config_data, f)\n",
    "\n",
    "print(\"Created 'hardhat.yaml' config!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Command\n",
    "From within the `yolov5/` directory, you can run:\n",
    "```\n",
    "!python train.py --img 640 --batch 16 --epochs 30 \\\n",
    "  --data hardhat.yaml --weights yolov5s.pt \\\n",
    "  --name yolo_hardhat_exp\n",
    "```\n",
    "Explaining each argument:\n",
    "- **--img 640**: The image resolution for training.\n",
    "- **--batch 16**: Batch size (adjust based on GPU memory).\n",
    "- **--epochs 30**: Number of training epochs. Increase if you have enough time and data.\n",
    "- **--data hardhat.yaml**: Path to your dataset config file.\n",
    "- **--weights yolov5s.pt**: Starting from a pretrained YOLOv5 model.\n",
    "- **--name yolo_hardhat_exp**: Output folder name for results.\n",
    "\n",
    "During training, YOLOv5 will display training/validation losses, mAP (mean Average Precision), and more.\n"
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example command to run YOLOv5 training. This cell won't run if the data isn't properly set up.\n",
    "# But here's how it might look:\n",
    "\n",
    "if IN_COLAB:\n",
    "    %cd /content/yolov5\n",
    "    !python train.py --img 640 --batch 16 --epochs 5 --data hardhat.yaml --weights yolov5s.pt --name yolo_hardhat_exp\n",
    "else:\n",
    "    print(\"Run the YOLOv5 training script locally or in your environment.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring Training & Evaluating Results\n",
    "- **mAP@0.5**: The primary object detection metric. Closer to 1 means better.\n",
    "- **Precision / Recall**: Also measured for each class. Good to see whether the model is catching heads with and without helmets.\n",
    "\n",
    "During training, YOLOv5 logs metrics per epoch. After training finishes, you can look at `runs/train/yolo_hardhat_exp` for:\n",
    "- `results.png` plot of training/validation curves.\n",
    "- Best weights stored as `best.pt`.\n"
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Viewing results\n",
    "if IN_COLAB:\n",
    "    # display the training results image\n",
    "    from IPython.display import Image\n",
    "    exp_path = '/content/yolov5/runs/train/yolo_hardhat_exp'\n",
    "    results_image = os.path.join(exp_path, 'results.png')\n",
    "    if os.path.exists(results_image):\n",
    "        display(Image(filename=results_image))\n",
    "    else:\n",
    "        print(\"Results image not found.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing & Inference\n",
    "Use your trained model to predict on **unseen** images:\n",
    "```\n",
    "!python detect.py --weights runs/train/yolo_hardhat_exp/weights/best.pt \\\n",
    "                  --img 640 --conf 0.25 --source /content/HardHat_Dataset/val/images\n",
    "```\n",
    "This will create bounding box predictions in `runs/detect/exp/`.\n"
    ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example inference code\n",
    "if IN_COLAB:\n",
    "    %cd /content/yolov5\n",
    "    !python detect.py --weights runs/train/yolo_hardhat_exp/weights/best.pt --img 640 --conf 0.25 --source /content/HardHat_Dataset/val/images --name hardhat_inference\n",
    "else:\n",
    "    print(\"Run detection script locally.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Inference Results\n",
    "Check the `runs/detect/hardhat_inference` folder for images with bounding boxes over `helmet` or `head`.\n",
    "A typical bounding box label might read `helmet 0.91`, indicating the model is **91% confident** it's a helmet.\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tips for Improvement\n",
    "1. **More Data**: The model improves with more diverse training images (lighting, angles, worker positions, different backgrounds).\n",
    "2. **Longer Training**: 30–50 epochs or more, if you have GPU resources.\n",
    "3. **Hyperparameter Tuning**: YOLOv5 has advanced hyperparameters (e.g., mosaic augmentation, LR schedules).\n",
    "4. **Data Augmentation**: Flips, rotations, color jitter for robust performance.\n",
    "5. **Advanced Models**: YOLOv5-l, YOLOv7, or other object detection frameworks (Detectron2, mmdetection) might yield higher accuracy.\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Considerations\n",
    "- **Edge Deployment**: Running these models on site (in real-time) may require smaller, faster models or specialized hardware.\n",
    "- **False Positives/Negatives**: A missed detection of a worker without a helmet can have safety implications.\n",
    "- **Privacy & Ethics**: Worker monitoring must follow local regulations and respect privacy.\n",
    "- **Integration**: Alerts or logs can integrate with a construction management system.\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion & Next Steps\n",
    "\n",
    "In this notebook, you've:\n",
    "1. Explored how to set up YOLOv5 for **hardhat detection**.\n",
    "2. Learned basic steps of data preparation, training, and inference.\n",
    "3. Seen how to interpret object detection metrics (mAP, precision, recall).\n",
    "\n",
    "**Next Steps**:\n",
    "- Expand your dataset or gather your own site images.\n",
    "- Tune hyperparameters, try advanced YOLO versions or other detection frameworks.\n",
    "- Explore **live camera feed** integration if you want real-time detection on a construction site.\n",
    "- Keep refining the model, especially for edge cases (nighttime, partial occlusions, reflective surfaces).\n",
    "\n",
    "Deep learning can **dramatically** improve safety monitoring and compliance tracking for construction teams. Continue learning, stay curious, and best of luck in building a safer job site with AI!"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Resources & References**\n",
    "1. [YOLOv5 GitHub](https://github.com/ultralytics/yolov5)\n",
    "2. [Kaggle Hard Hat Detection](https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection)\n",
    "3. [Ultralytics Documentation](https://docs.ultralytics.com/) for YOLOv5 usage.\n",
    "4. [Albumentations Library](https://github.com/albumentations-team/albumentations) for data augmentation.\n",
    "5. Additional frameworks: [Detectron2 (Facebook AI)](https://github.com/facebookresearch/detectron2), [MMDetection](https://github.com/open-mmlab/mmdetection).\n",
    "\n",
    "___\n",
    "Feel free to modify any paths, hyperparameters, or code depending on your data and computing setup.  \n",
    "If you have questions, consult the YOLOv5 issues/discussions on GitHub or the Kaggle community forums."
    ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
